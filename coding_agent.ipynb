{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "GIT_TOKEN = os.getenv(\"GIT_TOKEN\")\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of our agent.\"\"\"\n",
    "    question: str\n",
    "    certainty_score: int\n",
    "    search_results: list\n",
    "    web_score: str\n",
    "    repo_name: str\n",
    "    generation: str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì§ˆë¬¸ì— ëŒ€í•œ LLMì˜ ë‹µë³€ ì‹ ë¢°ë„ ì ê²€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "def check_certainty(state: AgentState) -> AgentState:\n",
    "    \"\"\"Evaluate certainty score for the query.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    class CertaintyScoreResponse(BaseModel):\n",
    "        score: int = Field(description=\"Certainty score from 1 to 100. Higher is better.\")\n",
    "\n",
    "    certainty_score = llm.with_structured_output(CertaintyScoreResponse)\n",
    "\n",
    "    print(\"--- Checking LLM's Certainty ---\")\n",
    "    score_response = certainty_score.invoke(question)\n",
    "\n",
    "    return {\n",
    "        \"certainty_score\": score_response.score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_based_on_certainty(state: AgentState) -> Literal[\"web_search\", \"direct_response\"]:\n",
    "    \"\"\"Route to appropriate node based on certainty score.\"\"\"\n",
    "    score = state[\"certainty_score\"]\n",
    "\n",
    "    if score != 100:\n",
    "        print(\"--- LLM is not certain so It will do web Search ---\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"--- LLM is certain so It will generate answer directly ---\")\n",
    "        return \"direct_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking LLM's Certainty ---\n",
      "{'certainty_score': 85}\n"
     ]
    }
   ],
   "source": [
    "question = \"Langgraphë¡œ ragë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•\"\n",
    "\n",
    "score_print = check_certainty({\"question\": question})\n",
    "print(score_print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLMì´ ìŠ¤ìŠ¤ë¡œ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ê²½ìš°ì˜ ë…¸ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_response(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "    result = llm.invoke(question)\n",
    "    return {\"generation\": result.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì›¹ ê²€ìƒ‰ ê¸°ë°˜ì˜ ë‹µë³€ ê°€ëŠ¥ ì—¬ë¶€ íŒë‹¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "def web_search(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Perform web search and evaluate results.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    search_tool = TavilySearchResults(max_results=3)\n",
    "    search_results = search_tool.invoke(question)\n",
    "\n",
    "    class answer_available(BaseModel):\n",
    "        \"\"\"Binary score for answer availability.\"\"\"\n",
    "        binary_score: str = Field(description=\"\"\"\n",
    "                                    If web search result can solve the user's ask, answer 'yes'.\n",
    "                                    If not, answer 'no'\"\"\")\n",
    "    \n",
    "    evaluator = llm.with_structured_output(answer_available)\n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Evaluate if these search results can answer the user's question with a simple yes/no.\"),\n",
    "        (\"user\", \"\"\"\n",
    "         Question: {question}\n",
    "         Seach Result: {results}\n",
    "         Can these results answer the question adequately?\n",
    "         \"\"\")\n",
    "    ])\n",
    "\n",
    "    print(\"--- Check whether web search is sufficient for user's ask ---\")\n",
    "\n",
    "    evaluation = evaluator.invoke(\n",
    "        eval_prompt.format(\n",
    "            question=question, results=\"\\n\".join(f\"- {result['content']}\" for result in search_results)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"search_results\": search_results,\n",
    "        \"web_score\": evaluation.binary_score\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì›¹ ê²€ìƒ‰ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥í•œì§€ ì—¬ë¶€ íŒë‹¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Check whether web search is sufficient for user's ask ---\n",
      "{'search_results': [{'url': 'https://github.com/Marker-Inc-Korea/AutoRAG/blob/main/docs/source/install.md', 'content': 'To install AutoRAG, you can use pip:\\n\\nPlus, it is recommended to install PyOpenSSL and nltk libraries for full features.\\n\\nNote for Windows Users\\n\\nAutoRAG is not fully supported on Windows yet. There are several constraints for Windows users.\\n\\nDue to the constraints, we recommend using Docker images for running AutoRAG on Windows.\\n\\nPlus, you MAKE SURE UPGRADE UP TO v0.3.1 for Windows users.\\n\\nInstallation for Local Models ğŸ  [...] For using local models, you need to install some additional dependencies.\\n\\nInstallation for Parsing ğŸŒ²\\n\\nFor parsing you need to install some local packages like libmagic,\\ntesseract, and poppler.\\nThe installation method depends upon your OS.\\n\\nAfter installing this, you can install AutoRAG with parsing like below.\\n\\nInstallation for Korean ğŸ‡°ğŸ‡·\\n\\nYou can install optional dependencies for the Korean language. [...] Installation and Setup'}, {'url': 'https://github.com/ictnlp/Auto-RAG', 'content': 'Installation. Environment requirements: Python 3.12, FlexRAG. conda env create autorag python=3.12 pip install flexrag.'}, {'url': 'https://github.com/Marker-Inc-Korea/AutoRAG-tutorial-ko', 'content': 'AutoRAG í•œêµ­ì–´ íŠœí† ë¦¬ì–¼ì„ ìœ„í•œ ë ˆí¬ì…ë‹ˆë‹¤.\\nì´ ë ˆí¬ì—ì„œëŠ” ì•„ì£¼ ê°„ë‹¨í•œ ë°ì´í„°ë¥¼ í†µí•´ì„œ AutoRAGë¥¼ ì‹¤í–‰í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nìœ íŠœë¸Œ ì˜ìƒ\\n\\ní•´ë‹¹ ë ˆí¬ë¥¼ ì´ìš©í•œ íŠœí† ë¦¬ì–¼ ì˜ìƒì…ë‹ˆë‹¤.\\n\\n\\n\\nì„¤ì¹˜\\n\\nìœ„ë¥¼ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ AutoRAGê°€ ì„¤ì¹˜ë©ë‹ˆë‹¤.\\n\\në¨¼ì €, OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì§ì ‘ ì„¤ì •í•˜ê±°ë‚˜ .env íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ê·¸ ì•ˆì— ê¸°ì…í•˜ì„¸ìš”.\\n\\nRAG í‰ê°€ ë°ì´í„°ì…‹ ì œì‘ íŠœí† ë¦¬ì–¼\\n\\nAutoRAG ì‚¬ìš©ì„ ìœ„í•˜ì—¬ ë¨¼ì € RAG í‰ê°€ ë°ì´í„°ì…‹ì„ ì œì‘í•´ì•¼ í•©ë‹ˆë‹¤. ì•„ë˜ ê³¼ì •ì„ í†µí•´ ì§ì ‘ ë°ì´í„°ì…‹ì„ ì œì‘í•˜ê³  ì‚¬ìš©í•´ë³´ì„¸ìš”.\\n\\ní”„ë¡œì íŠ¸ êµ¬ë™\\n\\nmain.py ì´ìš©\\n\\ncli ì´ìš©\\n\\nìœ„ ë°ì´í„°ì…‹ íŠœí† ë¦¬ì–¼ì—ì„œ ì œì‘í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‹¤í–‰í•˜ë ¤ë©´,\\ncorpus.parquetì„ corpus_new.parquet, qa.parquetì„ qa_new.parquetìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.\\n4. benchmark í´ë”ê°€ ìƒì„±ë˜ë©´ ê±°ê¸°ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\nëŒ€ì‹œë³´ë“œ ì‹¤í–‰'}], 'web_score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "question = \"autorag githubì—ì„œ ëª…ì‹œëœ ì„¤ì¹˜ ë°©ë²•ì„ ì•Œë ¤ì¤˜\"\n",
    "\n",
    "result = web_search({\"question\": question})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¡œ í•´ê²° ê°€ëŠ¥/ë¶ˆê°€ëŠ¥ ì—¬ë¶€ë¡œ ë‹¤ìŒ ë…¸ë“œ ë¼ìš°íŒ…í•˜ëŠ” í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_search(state: AgentState) -> Literal[\"generate\", \"github_search\"]:\n",
    "    \"\"\"\n",
    "    Route based on search evaluation.\n",
    "    \"\"\"\n",
    "    if state[\"web_score\"] == \"yes\":\n",
    "        print(\"--- ì›¹ê²€ìƒ‰ ê²°ê³¼ë¡œ í•´ê²° ê°€ëŠ¥ ---\")\n",
    "        return \"web_generate\"\n",
    "    else:\n",
    "        print(\"--- ì›¹ê²€ìƒ‰ ê²°ê³¼ë¡œ í•´ê²° ë¶ˆê°€, ê¹ƒí—ˆë¸Œ ê²€ìƒ‰ ì§„í–‰ ---\")\n",
    "        return \"github_search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì›¹ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€ ë…¸ë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def web_generate(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "    web_results = state[\"search_results\"]\n",
    "\n",
    "    def format_web_results(results):\n",
    "        formatted = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            formatted.append(f\"Source {i}: \\nURL: {result['url']}\\nContent: {result['content']}\\n\")\n",
    "        return \"\\n\".join(formatted)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant that generates comprehensive answers based on web search results.\n",
    "        Use the provided search results to answer the user's question.\n",
    "        Make sure to synthesize information from multiple sources when possible.\n",
    "        If the search results don't contain enough information to fully answer the question, acknowledge this limitation.\"\"\"),\n",
    "        (\"user\", \"\"\"Question: {question}\n",
    "\n",
    "        Search Results:\n",
    "        {web_results}\n",
    "\n",
    "        Please provide a detailed answer based on these search results. Answer in Korean\"\"\")\n",
    "    ])\n",
    "    chain = (\n",
    "        {\n",
    "            \"question\": lambda x: x[\"question\"], \n",
    "            \"web_results\": lambda x: format_web_results(x[\"search_results\"])\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    print(\"--- ì›¹ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ ìƒì„±ì¤‘ ---\")\n",
    "    response = chain.invoke({\n",
    "      \"question\": question,\n",
    "      \"web_results\": web_results  \n",
    "    })\n",
    "\n",
    "    return {\"generation\": response}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ê¹ƒí—™ ë ˆí¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import GithubFileLoader\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def git_loader(repo, branch_name):\n",
    "    loader = GithubFileLoader(\n",
    "        repo=repo,\n",
    "        branch=branch_name,\n",
    "        access_token=GIT_TOKEN,\n",
    "        github_api_url=\"https://api.github.com\",\n",
    "        file_filter=lambda file_path: file_path.endswith(\".md\"),    # ë‚´ê°€ ê°€ì ¸ì˜¬ íŒŒì¼ í™•ì¥ìì\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "\n",
    "def git_vector_embedding(repo_name):\n",
    "    client = chromadb.Client(Settings(\n",
    "        is_persistent=True,\n",
    "        persis_directory=\"./chroma_db\"  # ì €ì¥ë  ë””ë ‰í† ë¦¬ ì§€ì •\n",
    "    ))\n",
    "\n",
    "    collection_name = repo_name.split(\"/\")[1]\n",
    "\n",
    "    existing_collections = client.list_collections()\n",
    "    if collection_name in [col.name for col in existing_collections]:\n",
    "        print(f\"--- Loading existing collection for {collection_name} ---\")\n",
    "        vectorstore = Chroma(\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=OpenAIEmbeddings(),\n",
    "        )\n",
    "    else:\n",
    "        print(f\"--- Creating new collection for {collection_name} ---\")\n",
    "        try:\n",
    "            git_docs = git_loader(repo_name, \"master\")\n",
    "        except:\n",
    "            git_docs = git_loader(repo_name, \"main\")\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=500, chunk_overlap=50\n",
    "        )\n",
    "        docs_splits = text_splitter.split_documents(git_docs)\n",
    "\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=docs_splits,\n",
    "            collection_name=collection_name,\n",
    "            embedding=OpenAIEmbeddings(),\n",
    "            client=client\n",
    "        )\n",
    "\n",
    "    return vectorstore\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
