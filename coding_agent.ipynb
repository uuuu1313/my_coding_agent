{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "GIT_TOKEN = os.getenv(\"GIT_TOKEN\")\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"The state of our agent.\"\"\"\n",
    "    question: str\n",
    "    certainty_score: int\n",
    "    search_results: list\n",
    "    web_score: str\n",
    "    repo_name: str\n",
    "    generation: str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 질문에 대한 LLM의 답변 신뢰도 점검"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "def check_certainty(state: AgentState) -> AgentState:\n",
    "    \"\"\"Evaluate certainty score for the query.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    class CertaintyScoreResponse(BaseModel):\n",
    "        score: int = Field(description=\"Certainty score from 1 to 100. Higher is better.\")\n",
    "\n",
    "    certainty_score = llm.with_structured_output(CertaintyScoreResponse)\n",
    "\n",
    "    print(\"--- Checking LLM's Certainty ---\")\n",
    "    score_response = certainty_score.invoke(question)\n",
    "\n",
    "    return {\n",
    "        \"certainty_score\": score_response.score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_based_on_certainty(state: AgentState) -> Literal[\"web_search\", \"direct_response\"]:\n",
    "    \"\"\"Route to appropriate node based on certainty score.\"\"\"\n",
    "    score = state[\"certainty_score\"]\n",
    "\n",
    "    if score != 100:\n",
    "        print(\"--- LLM is not certain so It will do web Search ---\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        print(\"--- LLM is certain so It will generate answer directly ---\")\n",
    "        return \"direct_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking LLM's Certainty ---\n",
      "{'certainty_score': 85}\n"
     ]
    }
   ],
   "source": [
    "question = \"Langgraph로 rag를 구축하는 방법\"\n",
    "\n",
    "score_print = check_certainty({\"question\": question})\n",
    "print(score_print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM이 스스로 답변할 수 있는 경우의 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_response(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "    result = llm.invoke(question)\n",
    "    return {\"generation\": result.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 검색 기반의 답변 가능 여부 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "def web_search(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Perform web search and evaluate results.\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    search_tool = TavilySearchResults(max_results=3)\n",
    "    search_results = search_tool.invoke(question)\n",
    "\n",
    "    class answer_available(BaseModel):\n",
    "        \"\"\"Binary score for answer availability.\"\"\"\n",
    "        binary_score: str = Field(description=\"\"\"\n",
    "                                    If web search result can solve the user's ask, answer 'yes'.\n",
    "                                    If not, answer 'no'\"\"\")\n",
    "    \n",
    "    evaluator = llm.with_structured_output(answer_available)\n",
    "    eval_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Evaluate if these search results can answer the user's question with a simple yes/no.\"),\n",
    "        (\"user\", \"\"\"\n",
    "         Question: {question}\n",
    "         Seach Result: {results}\n",
    "         Can these results answer the question adequately?\n",
    "         \"\"\")\n",
    "    ])\n",
    "\n",
    "    print(\"--- Check whether web search is sufficient for user's ask ---\")\n",
    "\n",
    "    evaluation = evaluator.invoke(\n",
    "        eval_prompt.format(\n",
    "            question=question, results=\"\\n\".join(f\"- {result['content']}\" for result in search_results)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"search_results\": search_results,\n",
    "        \"web_score\": evaluation.binary_score\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 검색으로 해결 가능한지 여부 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Check whether web search is sufficient for user's ask ---\n",
      "{'search_results': [{'url': 'https://github.com/Marker-Inc-Korea/AutoRAG/blob/main/docs/source/install.md', 'content': 'To install AutoRAG, you can use pip:\\n\\nPlus, it is recommended to install PyOpenSSL and nltk libraries for full features.\\n\\nNote for Windows Users\\n\\nAutoRAG is not fully supported on Windows yet. There are several constraints for Windows users.\\n\\nDue to the constraints, we recommend using Docker images for running AutoRAG on Windows.\\n\\nPlus, you MAKE SURE UPGRADE UP TO v0.3.1 for Windows users.\\n\\nInstallation for Local Models 🏠 [...] For using local models, you need to install some additional dependencies.\\n\\nInstallation for Parsing 🌲\\n\\nFor parsing you need to install some local packages like libmagic,\\ntesseract, and poppler.\\nThe installation method depends upon your OS.\\n\\nAfter installing this, you can install AutoRAG with parsing like below.\\n\\nInstallation for Korean 🇰🇷\\n\\nYou can install optional dependencies for the Korean language. [...] Installation and Setup'}, {'url': 'https://github.com/ictnlp/Auto-RAG', 'content': 'Installation. Environment requirements: Python 3.12, FlexRAG. conda env create autorag python=3.12 pip install flexrag.'}, {'url': 'https://github.com/Marker-Inc-Korea/AutoRAG-tutorial-ko', 'content': 'AutoRAG 한국어 튜토리얼을 위한 레포입니다.\\n이 레포에서는 아주 간단한 데이터를 통해서 AutoRAG를 실행해 볼 수 있습니다.\\n\\n유튜브 영상\\n\\n해당 레포를 이용한 튜토리얼 영상입니다.\\n\\n\\n\\n설치\\n\\n위를 실행하면 자동으로 AutoRAG가 설치됩니다.\\n\\n먼저, OPENAI_API_KEY 환경 변수를 직접 설정하거나 .env 파일을 생성하여 그 안에 기입하세요.\\n\\nRAG 평가 데이터셋 제작 튜토리얼\\n\\nAutoRAG 사용을 위하여 먼저 RAG 평가 데이터셋을 제작해야 합니다. 아래 과정을 통해 직접 데이터셋을 제작하고 사용해보세요.\\n\\n프로젝트 구동\\n\\nmain.py 이용\\n\\ncli 이용\\n\\n위 데이터셋 튜토리얼에서 제작한 데이터셋으로 실행하려면,\\ncorpus.parquet을 corpus_new.parquet, qa.parquet을 qa_new.parquet으로 바꿔주세요.\\n4. benchmark 폴더가 생성되면 거기서 결과를 확인할 수 있습니다.\\n\\n대시보드 실행'}], 'web_score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "question = \"autorag github에서 명시된 설치 방법을 알려줘\"\n",
    "\n",
    "result = web_search({\"question\": question})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 검색 결과로 해결 가능/불가능 여부로 다음 노드 라우팅하는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_search(state: AgentState) -> Literal[\"generate\", \"github_search\"]:\n",
    "    \"\"\"\n",
    "    Route based on search evaluation.\n",
    "    \"\"\"\n",
    "    if state[\"web_score\"] == \"yes\":\n",
    "        print(\"--- 웹검색 결과로 해결 가능 ---\")\n",
    "        return \"web_generate\"\n",
    "    else:\n",
    "        print(\"--- 웹검색 결과로 해결 불가, 깃허브 검색 진행 ---\")\n",
    "        return \"github_search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹검색 기반 답변 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def web_generate(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "    web_results = state[\"search_results\"]\n",
    "\n",
    "    def format_web_results(results):\n",
    "        formatted = []\n",
    "        for i, result in enumerate(results, 1):\n",
    "            formatted.append(f\"Source {i}: \\nURL: {result['url']}\\nContent: {result['content']}\\n\")\n",
    "        return \"\\n\".join(formatted)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant that generates comprehensive answers based on web search results.\n",
    "        Use the provided search results to answer the user's question.\n",
    "        Make sure to synthesize information from multiple sources when possible.\n",
    "        If the search results don't contain enough information to fully answer the question, acknowledge this limitation.\"\"\"),\n",
    "        (\"user\", \"\"\"Question: {question}\n",
    "\n",
    "        Search Results:\n",
    "        {web_results}\n",
    "\n",
    "        Please provide a detailed answer based on these search results. Answer in Korean\"\"\")\n",
    "    ])\n",
    "    chain = (\n",
    "        {\n",
    "            \"question\": lambda x: x[\"question\"], \n",
    "            \"web_results\": lambda x: format_web_results(x[\"search_results\"])\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    print(\"--- 웹 검색 결과 기반 답변 생성중 ---\")\n",
    "    response = chain.invoke({\n",
    "      \"question\": question,\n",
    "      \"web_results\": web_results  \n",
    "    })\n",
    "\n",
    "    return {\"generation\": response}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 깃헙 레포 정보를 가져오는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import GithubFileLoader\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def git_loader(repo, branch_name):\n",
    "    loader = GithubFileLoader(\n",
    "        repo=repo,\n",
    "        branch=branch_name,\n",
    "        access_token=GIT_TOKEN,\n",
    "        github_api_url=\"https://api.github.com\",\n",
    "        file_filter=lambda file_path: file_path.endswith(\".md\"),    # 내가 가져올 파일 확장자자\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "\n",
    "def git_vector_embedding(repo_name):\n",
    "    client = chromadb.Client(Settings(\n",
    "        is_persistent=True,\n",
    "        persis_directory=\"./chroma_db\"  # 저장될 디렉토리 지정\n",
    "    ))\n",
    "\n",
    "    collection_name = repo_name.split(\"/\")[1]\n",
    "\n",
    "    existing_collections = client.list_collections()\n",
    "    if collection_name in [col.name for col in existing_collections]:\n",
    "        print(f\"--- Loading existing collection for {collection_name} ---\")\n",
    "        vectorstore = Chroma(\n",
    "            client=client,\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=OpenAIEmbeddings(),\n",
    "        )\n",
    "    else:\n",
    "        print(f\"--- Creating new collection for {collection_name} ---\")\n",
    "        try:\n",
    "            git_docs = git_loader(repo_name, \"master\")\n",
    "        except:\n",
    "            git_docs = git_loader(repo_name, \"main\")\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=500, chunk_overlap=50\n",
    "        )\n",
    "        docs_splits = text_splitter.split_documents(git_docs)\n",
    "\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=docs_splits,\n",
    "            collection_name=collection_name,\n",
    "            embedding=OpenAIEmbeddings(),\n",
    "            client=client\n",
    "        )\n",
    "\n",
    "    return vectorstore\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
